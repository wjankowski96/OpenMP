
#include <random>
#include<stdio.h>
#include<mpi.h>
#include <fstream>
#include <string>
#include <fstream>
#include <iostream>
#include <iomanip>
#include <fstream>

#define NUM_ROWS_A 300 
#define NUM_COLUMNS_A 300 
#define NUM_ROWS_B 300 
#define NUM_COLUMNS_B 300 
#define MASTER_TO_SLAVE_TAG 1 //znacznik dla komunikatów wysyłanych z urządzenia nadrzędnego do urządzeń podrzędnych
#define SLAVE_TO_MASTER_TAG 4 //znacznik dla komunikatów wysyłanych z urządzeń podrzędnych do urządzenia nadrzędnego
void makeAB(); 
void printArray(); 

int rank; //Pobiera identyfikator procesu
int size;//Pobiera ilość dostępnych procesów
int i, j, k; 
double mat_a[NUM_ROWS_A][NUM_COLUMNS_A]; //deklaracja wejscia  [A]
double mat_b[NUM_ROWS_B][NUM_COLUMNS_B]; //deklaracja wejscia [B]
double mat_result[NUM_ROWS_A][NUM_COLUMNS_B]; //deklaracja wejscia [C]
double start_time; 
double end_time; 
int low_bound; //dolna granica liczby wierszy w zbiorze [A] przydzielonych jednostce podrzędnej
int upper_bound; //górna granica liczby wierszy w zbiorze [A] przydzielonych jednostce podrzędnej
int portion; //część liczby wierszy w zbiorze [A] przydzielonych jednostce podrzędnej
MPI_Status status; 
MPI_Request request; 

int main(int argc, char *argv[])
{

    MPI_Init(&argc, &argv); //initialize MPI 
    MPI_Comm_rank(MPI_COMM_WORLD, &rank); //get the rank
    MPI_Comm_size(MPI_COMM_WORLD, &size); //get number of processes

    
    if (rank == 0) {
        makeAB();
        start_time = MPI_Wtime();
        for (i = 1; i < size; i++) {//dla każdego watku podrzędnego innego niż główne
            portion = (NUM_ROWS_A / (size - 1)); // oblicz porcję bez wzorca

             printf("\nPortion wynosi = %d\n\n",portion);

            low_bound = (i - 1) * portion;

            printf("low_bound wynosi = %d\n",low_bound);
            
            if (((i + 1) == size) && ((NUM_ROWS_A % (size - 1)) != 0)) {//jeżeli rzędy [A] nie mogą być równo podzielone między pozostle procesory
                upper_bound = NUM_ROWS_A; //ostatni wątek otrzymuje wszystkie pozostałe wiersze
            } else {
                upper_bound = low_bound + portion; //rzędy [A] są równo podzielne między reszte watkow
            }
            //wysyła najpierw sygnał o niskiej wartości granicznej bez blokowania do urządzenia podrzędnego, do którego ma być wysłany
            MPI_Isend(&low_bound, 1, MPI_INT, i, MASTER_TO_SLAVE_TAG, MPI_COMM_WORLD, &request);
            //następnie wysyła górną granicę bez blokowania do docelowego urządzenia podrzędnego
            MPI_Isend(&upper_bound, 1, MPI_INT, i, MASTER_TO_SLAVE_TAG + 1, MPI_COMM_WORLD, &request);
            //w końcu wysyła przydzieloną część wiersza [A] bez blokowania do docelowego urządzenia podrzędnego
            MPI_Isend(&mat_a[low_bound][0], (upper_bound - low_bound) * NUM_COLUMNS_A, MPI_DOUBLE, i, MASTER_TO_SLAVE_TAG + 2, MPI_COMM_WORLD, &request);
        }
    }
/*Podczas MPI_Isend zawartość bufora (np. tablica ints) musi zostać odczytana i wysłana.
 W międzyczasie można nałożyć na toczący się proces jakieś obliczenia, jednak nie mogą one zmieniać (ani odczytywać)
 zawartości bufora send/recv. */

    //rozesłać [B] do wszystkich urządzeń podrzędnych
    MPI_Bcast(&mat_b, NUM_ROWS_B*NUM_COLUMNS_B, MPI_DOUBLE, 0, MPI_COMM_WORLD);

    /* praca wykonywana przez watki*/
    if (rank > 0) {
        //odbieranie niskich wartości granicznych od urządzenia nadrzędnego
        MPI_Recv(&low_bound, 1, MPI_INT, 0, MASTER_TO_SLAVE_TAG, MPI_COMM_WORLD, &status);
        //następne otrzymanie górnej granicy od mastera
        MPI_Recv(&upper_bound, 1, MPI_INT, 0, MASTER_TO_SLAVE_TAG + 1, MPI_COMM_WORLD, &status);
        //w końcu otrzymuje od nadrzędnego część wiersza [A] do przetworzenia
        MPI_Recv(&mat_a[low_bound][0], (upper_bound - low_bound) * NUM_COLUMNS_A, MPI_DOUBLE, 0, MASTER_TO_SLAVE_TAG + 2, MPI_COMM_WORLD, &status);
        for (i = low_bound; i < upper_bound; i++) {//iteracja przez dany zbiór wierszy [A].
            for (j = 0; j < NUM_COLUMNS_B; j++) {//iteracja po kolumnach [B]
                for (k = 0; k < NUM_ROWS_B; k++) {//iteracja po wierszach [B]
                    mat_result[i][j] += (mat_a[i][k] * mat_b[k][j]);
                }
            }
        }


 std::fstream file4;
     file4.open ("nazwa.csv", std::ios::out | std::ios::app);
     if (file4) {
        
         file4 << "Liczba kolumn: "<<NUM_COLUMNS_B<<std::endl;
        
         for(int i = 0; i < NUM_ROWS_B; i++){
             for(int j = 0; j < NUM_COLUMNS_B; j++)
             {
                 file4 <<std::setprecision(12)<< mat_result[i][j]<<";";
             }
             file4 <<  std::endl;
         }
     }



        //najpierw bez blokowania odsyłają do urządzenia nadrzędnego sygnał o niskiej wartości granicznej
        MPI_Isend(&low_bound, 1, MPI_INT, 0, SLAVE_TO_MASTER_TAG, MPI_COMM_WORLD, &request);
        //przesyłają górną granicę do mastera w następnej kolejności bez blokowania
        MPI_Isend(&upper_bound, 1, MPI_INT, 0, SLAVE_TO_MASTER_TAG + 1, MPI_COMM_WORLD, &request);
        //w końcu wysyła przetworzoną porcję danych bez blokowania do urządzenia nadrzędnego
        MPI_Isend(&mat_result[low_bound][0], (upper_bound - low_bound) * NUM_COLUMNS_B, MPI_DOUBLE, 0, SLAVE_TO_MASTER_TAG + 2, MPI_COMM_WORLD, &request);
    }

    /* master gathers processed work*/
    if (rank == 0) {
        for (i = 1; i < size; i++) {// dopóki wszystkie urządzenia podrzędne nie oddadzą przetworzonych danych
            //odebrać sygnał o niskiej wartości granicznej od urządzenia podrzędnego
            MPI_Recv(&low_bound, 1, MPI_INT, i, SLAVE_TO_MASTER_TAG, MPI_COMM_WORLD, &status);
            //odebrać górną granicę od urządzenia podrzędnego
            MPI_Recv(&upper_bound, 1, MPI_INT, i, SLAVE_TO_MASTER_TAG + 1, MPI_COMM_WORLD, &status);
            //odbieranie przetworzonych danych od urządzenia podrzędnego
            MPI_Recv(&mat_result[low_bound][0], (upper_bound - low_bound) * NUM_COLUMNS_B, MPI_DOUBLE, i, SLAVE_TO_MASTER_TAG + 2, MPI_COMM_WORLD, &status);
        }
        end_time = MPI_Wtime();
        printf("\nRunning Time = %f\n\n", end_time - start_time);
        //printArray();
    }
    MPI_Finalize(); 
    return 0;
}

void makeAB()
{

    for (i = 0; i < NUM_ROWS_A; i++)
    {
        for (j = 0; j < NUM_COLUMNS_A; j++)
        {
            double fMin = 0, fMax = 100;
            double f = (double)rand() / RAND_MAX;
            mat_a[i][j] = fMin + f * (fMax - fMin);
        }
    }
    for (i = 0; i < NUM_ROWS_B; i++)
    {
        for (j = 0; j < NUM_COLUMNS_B; j++)
        {
            double fMin = 0, fMax = 100;
            double f = (double)rand() / RAND_MAX;
            mat_b[i][j] = fMin + f * (fMax - fMin);
        }
    }
}

void printArray()
{
    for (i = 0; i < NUM_ROWS_A; i++)
    {
        printf("\n");
        for (j = 0; j < NUM_COLUMNS_A; j++)
            printf("%8.2f  ", mat_a[i][j]);
    }
    printf("\n\n\n");
    for (i = 0; i < NUM_ROWS_B; i++)
    {
        printf("\n");
        for (j = 0; j < NUM_COLUMNS_B; j++)
            printf("%8.2f  ", mat_b[i][j]);
    }
    printf("\n\n\n");
    for (i = 0; i < NUM_ROWS_A; i++)
    {
        printf("\n");
        for (j = 0; j < NUM_COLUMNS_B; j++)
            printf("%8.2f  ", mat_result[i][j]);
    }
    printf("\n\n");
}


